{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299f6003",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b50a268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1+cu116\n",
      "CUDA Available: True\n",
      "CUDA Device: NVIDIA GeForce GTX 1660 SUPER\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba57b1",
   "metadata": {},
   "source": [
    "__Configuration__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e61a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    BATCH_SIZE = 8  # Adjust for memory\n",
    "    IMG_HEIGHT = 256\n",
    "    IMG_WIDTH = 256\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-4\n",
    "    DATASET_PATH = './datasets'  # <- ensure correct root\n",
    "    DEVICE = device\n",
    "    NUM_WORKERS = 2  # Set to 0 on Kaggle if hitting RAM issues\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31405ffb",
   "metadata": {},
   "source": [
    "__Extract red mask from OVERLAY path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d6fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_red_mask_from_path(mask_path, width, height):\n",
    "    \"\"\"Read a color overlay image and extract the red-filled lesion as a binary mask {0,1}       Returns None if path missing/unreadable.\n",
    "    \"\"\"\n",
    "    if mask_path is None or (isinstance(mask_path, str) and not os.path.exists(mask_path)):\n",
    "        return None\n",
    "    if not isinstance(mask_path, str):\n",
    "        return None\n",
    "    overlay = cv2.imread(mask_path)  # BGR\n",
    "    if overlay is None:\n",
    "        return None\n",
    "    hsv = cv2.cvtColor(overlay, cv2.COLOR_BGR2HSV)\n",
    "    lower1, upper1 = np.array([0, 50, 50]),  np.array([10, 255, 255])\n",
    "    lower2, upper2 = np.array([170, 50, 50]), np.array([180, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower1, upper1) | cv2.inRange(hsv, lower2, upper2)\n",
    "    mask = (mask > 0).astype(np.float32)\n",
    "    mask = cv2.resize(mask, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "    mask = (mask > 0.5).astype(np.float32)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec140a6",
   "metadata": {},
   "source": [
    "__Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568f0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrokeDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths  # may contain None for \"Normal\" images\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # --- image ---\n",
    "        img = cv2.imread(self.image_paths[idx])\n",
    "        if img is None:\n",
    "            img = np.zeros((config.IMG_HEIGHT, config.IMG_WIDTH, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (config.IMG_WIDTH, config.IMG_HEIGHT), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # --- overlay -> binary mask (0/1); for Normal (no OVERLAY) use all-zeros ---\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        msk = extract_red_mask_from_path(mask_path, config.IMG_WIDTH, config.IMG_HEIGHT)\n",
    "        if msk is None:\n",
    "            msk = np.zeros((config.IMG_HEIGHT, config.IMG_WIDTH), dtype=np.float32)\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=img, mask=msk)\n",
    "            img, msk = transformed['image'], transformed['mask']\n",
    "\n",
    "        return img, msk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17028b2e",
   "metadata": {},
   "source": [
    "__Transforms__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332f8630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(is_training=True):\n",
    "    if is_training:\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=0.3),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=5, p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea570fab",
   "metadata": {},
   "source": [
    "__Data discovery (includes Normal with empty masks)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68b069ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"Scan dataset folders and pair PNG with OVERLAY having the same filename.\n",
    "       If a PNG has no matching OVERLAY (e.g., in Normal), include it with an empty mask.\n",
    "    \"\"\"\n",
    "    images_paths, masks_paths = [], []\n",
    "\n",
    "    # Expect subfolders: Bleeding/, Ischemia/, Normal/, maybe External_Test/\n",
    "    for class_dir in [\"Bleeding\", \"Ischemia\", \"Normal\"]:\n",
    "        png_dir = os.path.join(config.DATASET_PATH, class_dir, \"PNG\")\n",
    "        overlay_dir = os.path.join(config.DATASET_PATH, class_dir, \"OVERLAY\")\n",
    "        if not os.path.exists(png_dir):\n",
    "            continue\n",
    "        png_files = sorted(glob(os.path.join(png_dir, \"*.png\")))\n",
    "\n",
    "        for png_file in png_files:\n",
    "            filename = os.path.basename(png_file)\n",
    "            overlay_file = os.path.join(overlay_dir, filename)\n",
    "            if os.path.exists(overlay_dir) and os.path.exists(overlay_file):\n",
    "                images_paths.append(png_file)\n",
    "                masks_paths.append(overlay_file)\n",
    "            else:\n",
    "                # No overlay -> treat as Normal/negative (all-zero mask)\n",
    "                images_paths.append(png_file)\n",
    "                masks_paths.append(None)\n",
    "\n",
    "    print(f\"Found {len(images_paths)} images; of which {sum(1 for m in masks_paths if m is not None)} have overlays.\")\n",
    "    if len(images_paths) > 0:\n",
    "        print(f\"Sample image: {images_paths[0]}\")\n",
    "        print(f\"Sample mask : {masks_paths[0]}\")\n",
    "    return images_paths, masks_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfd068",
   "metadata": {},
   "source": [
    "__Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b88a5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Use logits (activation=None) + BCEWithLogitsLoss for numerical stability\n",
    "    model = smp.Unet(\n",
    "        encoder_name='efficientnet-b4',\n",
    "        encoder_weights='imagenet',\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "        activation=None,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6967281c",
   "metadata": {},
   "source": [
    "__Losses & Metrics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce724e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, y_pred_prob, y_true):\n",
    "        y_pred = y_pred_prob.view(-1)\n",
    "        y_true = y_true.view(-1)\n",
    "        intersection = (y_pred * y_true).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "    def forward(self, y_pred_logits, y_true):\n",
    "        prob = torch.sigmoid(y_pred_logits)\n",
    "        return 0.5 * self.bce(y_pred_logits, y_true) + 0.5 * self.dice(prob, y_true)\n",
    "\n",
    "def calculate_dice_score(y_pred_prob, y_true, smooth=1e-6):\n",
    "    y_pred = (y_pred_prob > 0.5).float()\n",
    "    intersection = (y_pred * y_true).sum()\n",
    "    dice = (2. * intersection + smooth) / (y_pred.sum() + y_true.sum() + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "def calculate_iou_score(y_pred_prob, y_true, smooth=1e-6):\n",
    "    y_pred = (y_pred_prob > 0.5).float()\n",
    "    intersection = (y_pred * y_true).sum()\n",
    "    union = y_pred.sum() + y_true.sum() - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.item()\n",
    "\n",
    "def calculate_sensitivity(y_pred_prob, y_true):\n",
    "    y_pred = (y_pred_prob > 0.5).float()\n",
    "    true_positives = (y_pred * y_true).sum()\n",
    "    actual_positives = y_true.sum()\n",
    "    if actual_positives == 0:\n",
    "        return 0.0\n",
    "    return (true_positives / actual_positives).item()\n",
    "\n",
    "def calculate_specificity(y_pred_prob, y_true):\n",
    "    y_pred = (y_pred_prob > 0.5).float()\n",
    "    true_negatives = ((1 - y_pred) * (1 - y_true)).sum()\n",
    "    actual_negatives = (1 - y_true).sum()\n",
    "    if actual_negatives == 0:\n",
    "        return 0.0\n",
    "    return (true_negatives / actual_negatives).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e242c",
   "metadata": {},
   "source": [
    "__Train / Validate loops__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1764c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = running_dice = running_iou = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    for batch_idx, (images, masks) in enumerate(progress_bar):\n",
    "        images = images.to(device)\n",
    "        masks = masks.unsqueeze(1).to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, masks)\n",
    "\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        dice_score = calculate_dice_score(probs, masks)\n",
    "        iou_score = calculate_iou_score(probs, masks)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_dice += dice_score\n",
    "        running_iou += iou_score\n",
    "\n",
    "        progress_bar.set_postfix({'Loss': f'{loss.item():.4f}', 'Dice': f'{dice_score:.4f}', 'IoU': f'{iou_score:.4f}'})\n",
    "\n",
    "    N = len(dataloader)\n",
    "    return running_loss / N, running_dice / N, running_iou / N\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = running_dice = running_iou = 0.0\n",
    "    running_sensitivity = running_specificity = running_accuracy = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc='Validation')\n",
    "    with torch.no_grad():\n",
    "        for images, masks in progress_bar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.unsqueeze(1).to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, masks)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            dice_score = calculate_dice_score(probs, masks)\n",
    "            iou_score = calculate_iou_score(probs, masks)\n",
    "            sensitivity = calculate_sensitivity(probs, masks)\n",
    "            specificity = calculate_specificity(probs, masks)\n",
    "\n",
    "            pred_binary = (probs > 0.5).float()\n",
    "            accuracy = (pred_binary == masks).float().mean().item()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_dice += dice_score\n",
    "            running_iou += iou_score\n",
    "            running_sensitivity += sensitivity\n",
    "            running_specificity += specificity\n",
    "            running_accuracy += accuracy\n",
    "\n",
    "            progress_bar.set_postfix({'Loss': f'{loss.item():.4f}', 'Dice': f'{dice_score:.4f}', 'IoU': f'{iou_score:.4f}'})\n",
    "\n",
    "    N = len(dataloader)\n",
    "    return (\n",
    "        running_loss / N,\n",
    "        running_dice / N,\n",
    "        running_iou / N,\n",
    "        running_sensitivity / N,\n",
    "        running_specificity / N,\n",
    "        running_accuracy / N,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb2147",
   "metadata": {},
   "source": [
    "__Training wrapper__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c80867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    print(\"Loading dataset...\")\n",
    "    image_paths, mask_paths = load_and_preprocess_data()\n",
    "    if len(image_paths) == 0:\n",
    "        print(\"No images found! Please check the dataset path.\")\n",
    "        return None, None\n",
    "\n",
    "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        image_paths, mask_paths, test_size=0.15, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Training samples: {len(train_images)}\")\n",
    "    print(f\"Validation samples: {len(val_images)}\")\n",
    "\n",
    "    train_dataset = StrokeDataset(train_images, train_masks, get_transforms(True))\n",
    "    val_dataset   = StrokeDataset(val_images,   val_masks,   get_transforms(False))\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
    "                                  num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "    val_dataloader   = DataLoader(val_dataset,   batch_size=config.BATCH_SIZE, shuffle=False,\n",
    "                                  num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    model = build_model().to(device)\n",
    "    print(\"\\nModel built successfully!\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    criterion = CombinedLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=8,\n",
    "                                                     min_lr=1e-7, verbose=True)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_dice': [], 'val_dice': [],\n",
    "        'train_iou':  [], 'val_iou':  [],\n",
    "        'val_sensitivity': [], 'val_specificity': [], 'val_accuracy': []\n",
    "    }\n",
    "\n",
    "    best_dice = 0.0\n",
    "    patience_counter = 0\n",
    "    patience = 15\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        train_loss, train_dice, train_iou = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "        val_loss, val_dice, val_iou, val_sensitivity, val_specificity, val_accuracy = validate_epoch(\n",
    "            model, val_dataloader, criterion, device\n",
    "        )\n",
    "\n",
    "        scheduler.step(val_dice)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_dice'].append(train_dice)\n",
    "        history['val_dice'].append(val_dice)\n",
    "        history['train_iou'].append(train_iou)\n",
    "        history['val_iou'].append(val_iou)\n",
    "        history['val_sensitivity'].append(val_sensitivity)\n",
    "        history['val_specificity'].append(val_specificity)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}, Train IoU: {train_iou:.4f}\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f}, Val   Dice: {val_dice:.4f}, Val   IoU: {val_iou:.4f}\")\n",
    "        print(f\"Val Sensitivity: {val_sensitivity:.4f}, Val Specificity: {val_specificity:.4f}\")\n",
    "        print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if val_dice > best_dice:\n",
    "            best_dice = val_dice\n",
    "            torch.save(model.state_dict(), 'best_stroke_model_with_normals.pth')\n",
    "            print(f\"New best model saved! Dice: {best_dice:.4f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load('best_stroke_model_with_normals.pth'))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e8e5e",
   "metadata": {},
   "source": [
    "__Plots__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1005bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    axes[0, 0].plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Loss'); axes[0, 0].set_xlabel('Epoch'); axes[0, 0].set_ylabel('Loss'); axes[0, 0].legend(); axes[0, 0].grid(True)\n",
    "\n",
    "    axes[0, 1].plot(history['train_dice'], label='Training Dice', linewidth=2)\n",
    "    axes[0, 1].plot(history['val_dice'], label='Validation Dice', linewidth=2)\n",
    "    axes[0, 1].set_title('Dice Coefficient'); axes[0, 1].set_xlabel('Epoch'); axes[0, 1].set_ylabel('Dice'); axes[0, 1].legend(); axes[0, 1].grid(True)\n",
    "\n",
    "    axes[0, 2].plot(history['train_iou'], label='Training IoU', linewidth=2)\n",
    "    axes[0, 2].plot(history['val_iou'], label='Validation IoU', linewidth=2)\n",
    "    axes[0, 2].set_title('IoU Score'); axes[0, 2].set_xlabel('Epoch'); axes[0, 2].set_ylabel('IoU'); axes[0, 2].legend(); axes[0, 2].grid(True)\n",
    "\n",
    "    axes[1, 0].plot(history['val_sensitivity'], label='Validation Sensitivity', linewidth=2)\n",
    "    axes[1, 0].set_title('Sensitivity (Recall)'); axes[1, 0].set_xlabel('Epoch'); axes[1, 0].set_ylabel('Sensitivity'); axes[1, 0].legend(); axes[1, 0].grid(True)\n",
    "\n",
    "    axes[1, 1].plot(history['val_specificity'], label='Validation Specificity', linewidth=2)\n",
    "    axes[1, 1].set_title('Specificity'); axes[1, 1].set_xlabel('Epoch'); axes[1, 1].set_ylabel('Specificity'); axes[1, 1].legend(); axes[1, 1].grid(True)\n",
    "\n",
    "    axes[1, 2].plot(history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[1, 2].set_title('Binary Accuracy'); axes[1, 2].set_xlabel('Epoch'); axes[1, 2].set_ylabel('Accuracy'); axes[1, 2].legend(); axes[1, 2].grid(True)\n",
    "\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1d80b",
   "metadata": {},
   "source": [
    "__Visualization & Evaluation (same mask logic; skip metrics if no GT)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a00412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(model, image_paths, mask_paths, num_samples=5):\n",
    "    model.eval()\n",
    "    transforms = get_transforms(False)\n",
    "    rows = min(num_samples, len(image_paths))\n",
    "    fig, axes = plt.subplots(rows, 4, figsize=(20, 5*rows))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(rows):\n",
    "            image = cv2.imread(image_paths[i])\n",
    "            if image is None:\n",
    "                print(f\"Could not load image: {image_paths[i]}\")\n",
    "                continue\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image_resized = cv2.resize(image_rgb, (config.IMG_WIDTH, config.IMG_HEIGHT))\n",
    "\n",
    "            mask_binary = extract_red_mask_from_path(mask_paths[i], config.IMG_WIDTH, config.IMG_HEIGHT)\n",
    "            has_gt = mask_binary is not None\n",
    "            if not has_gt:\n",
    "                mask_binary = np.zeros((config.IMG_HEIGHT, config.IMG_WIDTH), dtype=np.float32)\n",
    "\n",
    "            transformed = transforms(image=image_resized, mask=mask_binary)\n",
    "            image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "\n",
    "            pred_logits = model(image_tensor)[0, 0].cpu().numpy()\n",
    "            pred_prob = 1 / (1 + np.exp(-pred_logits))\n",
    "            pred_mask_binary = (pred_prob > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "            if has_gt:\n",
    "                pred_binary = (pred_prob > 0.5).astype(np.float32)\n",
    "                intersection = np.sum(mask_binary * pred_binary)\n",
    "                union = np.sum(mask_binary) + np.sum(pred_binary) - intersection\n",
    "                iou = intersection / (union + 1e-6) if union > 0 else 0\n",
    "                dice = (2 * intersection) / (np.sum(mask_binary) + np.sum(pred_binary) + 1e-6)\n",
    "                title3 = f'Prediction Binary\\nDice: {dice:.3f}, IoU: {iou:.3f}'\n",
    "            else:\n",
    "                title3 = 'Prediction Binary (no GT)'\n",
    "\n",
    "            axes[i, 0].imshow(image_resized); axes[i, 0].set_title(f'Original Image {i+1}'); axes[i, 0].axis('off')\n",
    "            axes[i, 1].imshow(mask_binary, cmap='gray'); axes[i, 1].set_title('Ground Truth Mask' if has_gt else 'No GT (Normal)'); axes[i, 1].axis('off')\n",
    "            axes[i, 2].imshow(pred_prob, cmap='gray'); axes[i, 2].set_title('Prediction (Prob)'); axes[i, 2].axis('off')\n",
    "            axes[i, 3].imshow(pred_mask_binary, cmap='gray'); axes[i, 3].set_title(title3); axes[i, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "def evaluate_model(model, image_paths, mask_paths):\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    model.eval()\n",
    "    transforms = get_transforms(False)\n",
    "    dice_scores, iou_scores = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (img_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Evaluating {i}/{len(image_paths)}\")\n",
    "\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image_resized = cv2.resize(image_rgb, (config.IMG_WIDTH, config.IMG_HEIGHT))\n",
    "\n",
    "            mask_binary = extract_red_mask_from_path(mask_path, config.IMG_WIDTH, config.IMG_HEIGHT)\n",
    "            if mask_binary is None:\n",
    "                # skip normals for quantitative metrics\n",
    "                continue\n",
    "\n",
    "            x = get_transforms(False)(image=image_resized, mask=mask_binary)['image'].unsqueeze(0).to(device)\n",
    "            pred_logits = model(x)[0, 0].cpu().numpy()\n",
    "            pred_prob = 1 / (1 + np.exp(-pred_logits))\n",
    "            pred_binary = (pred_prob > 0.5).astype(np.float32)\n",
    "\n",
    "            intersection = np.sum(mask_binary * pred_binary)\n",
    "            union = np.sum(mask_binary) + np.sum(pred_binary) - intersection\n",
    "            if union > 0:\n",
    "                iou = intersection / union\n",
    "                dice = (2 * intersection) / (np.sum(mask_binary) + np.sum(pred_binary))\n",
    "                dice_scores.append(dice)\n",
    "                iou_scores.append(iou)\n",
    "\n",
    "    print(\"\\nEvaluation Results (lesion slices only):\")\n",
    "    if len(dice_scores) == 0:\n",
    "        print(\"No GT masks found for evaluation.\")\n",
    "    else:\n",
    "        print(f\"Mean Dice Score: {np.mean(dice_scores):.4f} ± {np.std(dice_scores):.4f}\")\n",
    "        print(f\"Mean IoU  Score: {np.mean(iou_scores):.4f} ± {np.std(iou_scores):.4f}\")\n",
    "        print(f\"Median Dice Score: {np.median(dice_scores):.4f}\")\n",
    "        print(f\"Median IoU  Score: {np.median(iou_scores):.4f}\")\n",
    "    return dice_scores, iou_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662bc0",
   "metadata": {},
   "source": [
    "__Inference for a single image (no GT required)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0969bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(model, image_path, threshold=0.5, device=config.DEVICE):\n",
    "    model.eval()\n",
    "    transforms = get_transforms(False)\n",
    "\n",
    "    bgr = cv2.imread(image_path)\n",
    "    if bgr is None:\n",
    "        raise FileNotFoundError(f\"Could not read image: {image_path}\")\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    rgb = cv2.resize(rgb, (config.IMG_WIDTH, config.IMG_HEIGHT), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    sample = transforms(image=rgb, mask=np.zeros((config.IMG_HEIGHT, config.IMG_WIDTH), np.float32))\n",
    "    x = sample['image'].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0, 0].cpu().numpy()\n",
    "    prob = 1 / (1 + np.exp(-logits))\n",
    "    pred_bin = (prob > threshold).astype(np.uint8) * 255\n",
    "\n",
    "    overlay = rgb.copy()\n",
    "    overlay[pred_bin > 0] = (255, 64, 64)\n",
    "    return rgb, prob, pred_bin, overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e2761",
   "metadata": {},
   "source": [
    "__Main__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e2197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Brain Stroke Detection Training - PyTorch Version...\n",
      "============================================================\n",
      "Loading dataset...\n",
      "Found 6650 images; of which 2223 have overlays.\n",
      "Sample image: ./datasets\\Bleeding\\PNG\\10002.png\n",
      "Sample mask : ./datasets\\Bleeding\\OVERLAY\\10002.png\n",
      "Training samples: 5652\n",
      "Validation samples: 998\n",
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to C:\\Users\\Ishigami/.cache\\torch\\hub\\checkpoints\\efficientnet-b4-6ed6700e.pth\n",
      "100%|██████████| 74.4M/74.4M [00:14<00:00, 5.25MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model built successfully!\n",
      "Total parameters: 20,225,689\n",
      "Trainable parameters: 20,225,689\n",
      "Starting training...\n",
      "==================================================\n",
      "\n",
      "Epoch 1/50\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/707 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Brain Stroke Detection Training - PyTorch Version...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    model, history = train_model()\n",
    "\n",
    "    if model is not None and history is not None:\n",
    "        plot_training_history(history)\n",
    "        image_paths, mask_paths = load_and_preprocess_data()\n",
    "\n",
    "        print(\"\\nVisualizing predictions...\")\n",
    "        predict_and_visualize(model, image_paths[-10:], mask_paths[-10:], num_samples=5)\n",
    "\n",
    "        dice_scores, iou_scores = evaluate_model(model, image_paths[-100:], mask_paths[-100:])\n",
    "\n",
    "        print(\"\\nTraining completed successfully!\")\n",
    "        print(\"Model saved as 'best_stroke_model_with_normals.pth'\")\n",
    "    else:\n",
    "        print(\"Training failed. Please check the dataset path and try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
